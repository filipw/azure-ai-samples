{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"\"\"You are a medical classification engine for health conditions. Classify the prompt into into one of the following possible treatment options: 'doctor_required' (serious condition), 'pharmacist_required' (light condition) or 'rest_required' (general tiredness). If you cannot classify the prompt, output 'unknown'. \n",
    "Only respond with the single word classification. Do not produce any additional output.\n",
    "\n",
    "# Examples:\n",
    "User: \"I did not sleep well.\" Assistant: \"rest_required\"\n",
    "User: \"I chopped off my arm.\" Assistant: \"doctor_required\"\n",
    "\n",
    "# Task\n",
    "User: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_inputs = [\n",
    "    \"I'm tired.\", # rest_required\n",
    "    \"I'm bleeding from my eyes.\", # doctor_required\n",
    "    \"I have a headache.\" # pharmacist_required\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_OPENAI_RESOURCE = os.environ[\"AZURE_OPENAI_RESOURCE\"]\n",
    "AZURE_OPENAI_KEY = os.environ[\"AZURE_OPENAI_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first example shows using the inference client against an Azure OpenAI endpoint. In this case, three arguments are mandatory: \n",
    " * an endpoint URL in the form of `https://<resouce-name>.openai.azure.com/openai/deployments/<deployment-name>` \n",
    " * the credential to access it (could be either the key or the integrated Azure SDK authentication)\n",
    " * the API version (this is mandatory in Azure OpenAI API access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm tired. -> rest_required\n",
      "I'm bleeding from my eyes. -> doctor_required\n",
      "I have a headache. -> pharmacist_required\n"
     ]
    }
   ],
   "source": [
    "client = ChatCompletionsClient(\n",
    "    endpoint=f\"https://{AZURE_OPENAI_RESOURCE}.openai.azure.com/openai/deployments/gpt-4o-mini/\",\n",
    "    credential=AzureKeyCredential(AZURE_OPENAI_KEY),\n",
    "    api_version=\"2024-06-01\",\n",
    ")\n",
    "\n",
    "run_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference():\n",
    "    for user_input in user_inputs:\n",
    "        messages = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"{instruction}{user_input} Assistant: \"\n",
    "        }]\n",
    "        print(f\"{user_input} -> \", end=\"\")\n",
    "        stream = client.complete(\n",
    "            messages=messages,\n",
    "            stream=True\n",
    "        )\n",
    "        for chunk in stream:\n",
    "            if chunk.choices and chunk.choices[0].delta.content:\n",
    "                print(chunk.choices[0].delta.content, end=\"\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final example bootstraps a ChatCompletionsClient pointing at the local completion server from LM Studio. In this case, we do not need to supply the credentials as the server is running locally and we can access it without authentication.\n",
    "\n",
    "With this, the inference code is still the same, but everything happens completely locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm tired. ->  rest_required\n",
      "I'm bleeding from my eyes. ->  doctor_required\n",
      "I have a headache. ->  pharmacist_required\n"
     ]
    }
   ],
   "source": [
    "client = ChatCompletionsClient(\n",
    "    endpoint=\"http://localhost:1234/v1\",\n",
    "    credential=AzureKeyCredential(\"\")\n",
    ")\n",
    "\n",
    "run_inference()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
