{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook, we will explore the flexibility behind Azure AI Inference. This is the new [library](https://learn.microsoft.com/en-us/python/api/overview/azure/ai-inference-readme?view=azure-python-preview) from Azure, which allows us to run inference against a wide range of AI model deployments - both in Azure and, as we will see in this notebook, in other places as well.\n",
    "\n",
    "It is available for Python and for .NET - in this notebook, we will focus on the Python version. To begin with, we need to install the `azure.ai.inference` package. You can find the necessary dependencies in the accompanying `requirements.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to define a general task for our models. It will be a sample health problem classification, where the model will be asked to categorize user's input into one of four possible classes:\n",
    " - `doctor_required` - if the user should see a doctor immediately\n",
    " - `pharmacist_required` - if the user should see a pharmacist - for problems that can be solved with over-the-counter drugs\n",
    " - `rest_required` - if the user should rest and does not need professional help\n",
    " - `unknown` - if the model is not sure about the classification\n",
    "\n",
    "![](images/classification.excalidraw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"\"\"You are a medical classification engine for health conditions. Classify the prompt into into one of the following possible treatment options: 'doctor_required' (serious condition), 'pharmacist_required' (light condition) or 'rest_required' (general tiredness). If you cannot classify the prompt, output 'unknown'. \n",
    "Only respond with the single word classification. Do not produce any additional output.\n",
    "\n",
    "# Examples:\n",
    "User: \"I did not sleep well.\" Assistant: \"rest_required\"\n",
    "User: \"I chopped off my arm.\" Assistant: \"doctor_required\"\n",
    "\n",
    "# Task\n",
    "User: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need a set of sample inputs to the model, and the expected outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_inputs = [\n",
    "    \"I'm tired.\", # rest_required\n",
    "    \"I'm bleeding from my eyes.\", # doctor_required\n",
    "    \"I have a headache.\" # pharmacist_required\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inference code is very simple - we will call the `complete` method on the inference client, and indicate that we are interested in the streaming of the response. This way, we can process the response as it comes in, and not wait for the whole response to be ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference():\n",
    "    for user_input in user_inputs:\n",
    "        messages = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"{instruction}{user_input} Assistant: \"\n",
    "        }]\n",
    "        print(f\"{user_input} -> \", end=\"\")\n",
    "        stream = client.complete(\n",
    "            messages=messages,\n",
    "            stream=True\n",
    "        )\n",
    "        for chunk in stream:\n",
    "            if chunk.choices and chunk.choices[0].delta.content:\n",
    "                print(chunk.choices[0].delta.content, end=\"\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first example shows using the inference client against an Azure OpenAI endpoint. In this case, three arguments are mandatory: \n",
    " * an endpoint URL in the form of `https://<resouce-name>.openai.azure.com/openai/deployments/<deployment-name>` \n",
    " * the credential to access it (could be either the key or the integrated Azure SDK authentication)\n",
    " * the API version (this is mandatory in Azure OpenAI API access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_OPENAI_RESOURCE = os.environ[\"AZURE_OPENAI_RESOURCE\"]\n",
    "AZURE_OPENAI_KEY = os.environ[\"AZURE_OPENAI_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ChatCompletionsClient(\n",
    "    endpoint=f\"https://{AZURE_OPENAI_RESOURCE}.openai.azure.com/openai/deployments/gpt-4o-mini/\",\n",
    "    credential=AzureKeyCredential(AZURE_OPENAI_KEY),\n",
    "    api_version=\"2024-06-01\",\n",
    ")\n",
    "\n",
    "print(\" * AZURE OPENAI INFERENCE * \")\n",
    "run_inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second example will show how to use the inference client against a model hosted in [Github Models](https://github.com/marketplace/models). In order to make that, work, a Personal Access Token for Github is needed - the token does not need to have any permissions.\n",
    "\n",
    "In our case, we expect that the token is available in the env variable `GITHUB_TOKEN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GITHUB_TOKEN = os.environ[\"GITHUB_TOKEN\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Github Models, we can easily choose from a huge range of models, without having to deploy anything - so let's try running our task against `Llama-3.2-11B-Vision-Instruct`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ChatCompletionsClient(\n",
    "    endpoint=\"https://models.inference.ai.azure.com\",\n",
    "    credential=AzureKeyCredential(GITHUB_TOKEN),\n",
    "    model=\"Llama-3.2-11B-Vision-Instruct\"\n",
    ")\n",
    "\n",
    "print(\" * GITHUB MODELS INFERENCE * \")\n",
    "run_inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next example shows using the client against Azure AI model deployment. The prerequisite here is to have a model deployed as Serverless API or as a Managed Compute endpoint - the relevant instructions can be [found here](https://learn.microsoft.com/en-us/azure/ai-studio/concepts/deployments-overview#how-should-i-think-about-deployment-options).\n",
    "\n",
    "The two pieces of information needed to connect to such model are:\n",
    " * an endpoint URL in the form of `https://<deployment-name>.<region>.models.ai.azure.com` \n",
    " * the credential to access it (could be either the key or the integrated Azure SDK authentication)\n",
    "\n",
    "In our case we will read that information from the environment variables below, with the endpoint being explicitly split into the region and the deployment name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_AI_REGION = os.environ[\"AZURE_AI_REGION\"]\n",
    "AZURE_AI_DEPLOYMENT_NAME = os.environ[\"AZURE_AI_DEPLOYMENT_NAME\"]\n",
    "AZURE_AI_KEY = os.environ[\"AZURE_AI_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ChatCompletionsClient(\n",
    "    endpoint=f\"https://{AZURE_AI_DEPLOYMENT_NAME}.{AZURE_AI_REGION}.models.ai.azure.com\",\n",
    "    credential=AzureKeyCredential(AZURE_AI_KEY)\n",
    ")\n",
    "\n",
    "print(\" * AZURE AI INFERENCE * \")\n",
    "run_inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the fact that the library has \"Azure\" in its name, it is not restricted to models running in Azure (or Github, which is part of Microsoft). It can be used with any other model that is reachable over an HTTP compatible compatible with OpenAI API. \n",
    "\n",
    "This of course includes OpenAI itself. The next example shows, by just using the OpenAI API key, how to connect to the OpenAI model, in our case, the `gpt-4o-mini`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_KEY = os.environ[\"OPENAI_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ChatCompletionsClient(\n",
    "    endpoint=\"https://api.openai.com/v1\",\n",
    "    credential=AzureKeyCredential(OPENAI_KEY),\n",
    "    model=\"gpt-4o-mini\"\n",
    ")\n",
    "\n",
    "print(\" * OPENAI INFERENCE * \")\n",
    "run_inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final example bootstraps a `ChatCompletionsClient` pointing at the local completion server from [LM Studio](https://lmstudio.ai). In this case, we do not need to supply the credentials as the server is running locally and we can access it without authentication.\n",
    "\n",
    "In my case, I configured LM Studio to use `phi-3-mini-4k-instruct`.\n",
    "\n",
    "![](images/lmstudio.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ChatCompletionsClient(\n",
    "    endpoint=\"http://localhost:1234/v1\",\n",
    "    credential=AzureKeyCredential(\"\")\n",
    ")\n",
    "\n",
    "print(\" * LOCAL LM STUDIO SERVER INFERENCE * \")\n",
    "run_inference()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
